{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classify the images for dogs\n",
    "Load libs\n",
    "https://www.kaggle.com/code/khushikhushikhushi/dog-breed-classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-30 14:41:42.086521: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-30 14:41:42.944002: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/lib\n",
      "2024-06-30 14:41:42.944126: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2024-06-30 14:41:43.123996: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-06-30 14:41:45.368471: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/lib\n",
      "2024-06-30 14:41:45.368710: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/lib\n",
      "2024-06-30 14:41:45.368734: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential,load_model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical, plot_model\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "import cv2\n",
    "import datetime\n",
    "import timeit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper function to load images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images_from_folder(folder, image_size=(150, 150)):\n",
    "    images = []\n",
    "    labels = []\n",
    "    for label in os.listdir(folder):\n",
    "        label_path = os.path.join(folder, label)\n",
    "        if os.path.isdir(label_path):\n",
    "            for filename in os.listdir(label_path):\n",
    "                img_path = os.path.join(label_path, filename)\n",
    "                img = Image.open(img_path)\n",
    "                img = img.resize(image_size)\n",
    "                img = np.array(img)\n",
    "                images.append(img)\n",
    "                labels.append(label)\n",
    "    return np.array(images), np.array(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/home/prasad/09_personal/Coding_N_Work/2_ML_NN/Computer_Vision_data_Explore/Computer-Vision-Classify-Dogs/dataset_dogs/'\n",
    "image_size = (150, 150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_images_from_folder(data_dir, image_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize pixel values to be between 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode labels to integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert labels to one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = to_categorical(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split data into training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (773, 150, 150, 3)\n",
      "X_val shape: (194, 150, 150, 3)\n",
      "y_train shape: (773, 10)\n",
      "y_val shape: (194, 10)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Print shapes to confirm\n",
    "print(f'X_train shape: {X_train.shape}')\n",
    "print(f'X_val shape: {X_val.shape}')\n",
    "print(f'y_train shape: {y_train.shape}')\n",
    "print(f'y_val shape: {y_val.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-30 14:41:52.459518: E tensorflow/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2024-06-30 14:41:52.459757: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (XDEM-laptop): /proc/driver/nvidia/version does not exist\n",
      "2024-06-30 14:41:52.461447: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Conv2D(128, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Flatten(),\n",
    "    Dense(512, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(len(label_encoder.classes_), activation='softmax')\n",
    "])\n",
    "\n",
    "# here we use a bit of bigger pooling to extract broader features\n",
    "model1 = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)),\n",
    "    MaxPooling2D(3, 3),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(3, 3),\n",
    "    Conv2D(128, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(3, 3),\n",
    "    Flatten(),\n",
    "    Dense(512, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(len(label_encoder.classes_), activation='softmax')\n",
    "])\n",
    "\n",
    "# model.summary()\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model1.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up TensorBoard callback\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "25/25 [==============================] - ETA: 0s - loss: 2.6330 - accuracy: 0.1746"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-30 14:46:13.947589: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 4545576960 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 [==============================] - 46s 2s/step - loss: 2.6330 - accuracy: 0.1746 - val_loss: 2.1042 - val_accuracy: 0.3196\n",
      "Epoch 2/50\n",
      "25/25 [==============================] - ETA: 0s - loss: 1.4935 - accuracy: 0.5149"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-30 14:46:53.882113: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 4545576960 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 [==============================] - 39s 2s/step - loss: 1.4935 - accuracy: 0.5149 - val_loss: 0.9356 - val_accuracy: 0.7371\n",
      "Epoch 3/50\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.4697 - accuracy: 0.8771"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-30 14:47:37.568063: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 4545576960 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 [==============================] - 44s 2s/step - loss: 0.4697 - accuracy: 0.8771 - val_loss: 0.2424 - val_accuracy: 0.9433\n",
      "Epoch 4/50\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.1168 - accuracy: 0.9715"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-30 14:48:12.282601: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 4545576960 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 [==============================] - 34s 1s/step - loss: 0.1168 - accuracy: 0.9715 - val_loss: 0.0873 - val_accuracy: 0.9691\n",
      "Epoch 5/50\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0517 - accuracy: 0.9909"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-30 14:48:45.412810: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 4545576960 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 [==============================] - 33s 1s/step - loss: 0.0517 - accuracy: 0.9909 - val_loss: 0.0216 - val_accuracy: 1.0000\n",
      "Epoch 6/50\n",
      "25/25 [==============================] - 34s 1s/step - loss: 0.0166 - accuracy: 0.9961 - val_loss: 0.0033 - val_accuracy: 1.0000\n",
      "Epoch 7/50\n",
      "25/25 [==============================] - 31s 1s/step - loss: 0.0249 - accuracy: 0.9922 - val_loss: 0.0146 - val_accuracy: 1.0000\n",
      "Epoch 8/50\n",
      "25/25 [==============================] - 33s 1s/step - loss: 0.0195 - accuracy: 0.9987 - val_loss: 0.0082 - val_accuracy: 1.0000\n",
      "Epoch 9/50\n",
      "25/25 [==============================] - 34s 1s/step - loss: 0.0053 - accuracy: 1.0000 - val_loss: 4.2924e-05 - val_accuracy: 1.0000\n",
      "Epoch 10/50\n",
      "25/25 [==============================] - 33s 1s/step - loss: 8.5847e-04 - accuracy: 1.0000 - val_loss: 1.7578e-05 - val_accuracy: 1.0000\n",
      "Epoch 11/50\n",
      "25/25 [==============================] - 35s 1s/step - loss: 0.0111 - accuracy: 0.9961 - val_loss: 0.0030 - val_accuracy: 1.0000\n",
      "Epoch 12/50\n",
      "25/25 [==============================] - 34s 1s/step - loss: 0.0060 - accuracy: 0.9987 - val_loss: 0.0014 - val_accuracy: 1.0000\n",
      "Epoch 13/50\n",
      "25/25 [==============================] - 45s 2s/step - loss: 0.0130 - accuracy: 0.9935 - val_loss: 0.0145 - val_accuracy: 1.0000\n",
      "Epoch 14/50\n",
      "25/25 [==============================] - 38s 2s/step - loss: 0.0143 - accuracy: 0.9974 - val_loss: 0.0023 - val_accuracy: 1.0000\n",
      "Epoch 15/50\n",
      "25/25 [==============================] - 45s 2s/step - loss: 0.0205 - accuracy: 0.9922 - val_loss: 0.0019 - val_accuracy: 1.0000\n",
      "Epoch 16/50\n",
      "25/25 [==============================] - 39s 2s/step - loss: 0.0107 - accuracy: 0.9987 - val_loss: 2.4867e-04 - val_accuracy: 1.0000\n",
      "Epoch 17/50\n",
      "25/25 [==============================] - 40s 2s/step - loss: 0.0138 - accuracy: 0.9974 - val_loss: 0.0047 - val_accuracy: 1.0000\n",
      "Epoch 18/50\n",
      "25/25 [==============================] - 46s 2s/step - loss: 0.0106 - accuracy: 0.9974 - val_loss: 6.7048e-04 - val_accuracy: 1.0000\n",
      "Epoch 19/50\n",
      "25/25 [==============================] - 47s 2s/step - loss: 0.0117 - accuracy: 0.9987 - val_loss: 3.6695e-04 - val_accuracy: 1.0000\n",
      "Epoch 20/50\n",
      "25/25 [==============================] - 40s 2s/step - loss: 0.0226 - accuracy: 0.9922 - val_loss: 0.0029 - val_accuracy: 1.0000\n",
      "Epoch 21/50\n",
      "25/25 [==============================] - 35s 1s/step - loss: 0.0294 - accuracy: 0.9922 - val_loss: 0.0130 - val_accuracy: 1.0000\n",
      "Epoch 22/50\n",
      "25/25 [==============================] - 34s 1s/step - loss: 0.0181 - accuracy: 0.9948 - val_loss: 6.3493e-05 - val_accuracy: 1.0000\n",
      "Epoch 23/50\n",
      "25/25 [==============================] - 35s 1s/step - loss: 0.0140 - accuracy: 0.9961 - val_loss: 7.4332e-04 - val_accuracy: 1.0000\n",
      "Epoch 24/50\n",
      "25/25 [==============================] - 34s 1s/step - loss: 0.0130 - accuracy: 0.9948 - val_loss: 2.2919e-04 - val_accuracy: 1.0000\n",
      "Epoch 25/50\n",
      "25/25 [==============================] - 34s 1s/step - loss: 0.0130 - accuracy: 0.9948 - val_loss: 7.2227e-04 - val_accuracy: 1.0000\n",
      "Epoch 26/50\n",
      "25/25 [==============================] - 35s 1s/step - loss: 0.0181 - accuracy: 0.9948 - val_loss: 0.0027 - val_accuracy: 1.0000\n",
      "Epoch 27/50\n",
      "25/25 [==============================] - 35s 1s/step - loss: 0.0046 - accuracy: 0.9987 - val_loss: 2.9000e-04 - val_accuracy: 1.0000\n",
      "Epoch 28/50\n",
      "25/25 [==============================] - 36s 1s/step - loss: 0.0139 - accuracy: 0.9961 - val_loss: 4.7471e-04 - val_accuracy: 1.0000\n",
      "Epoch 29/50\n",
      "25/25 [==============================] - 34s 1s/step - loss: 0.0108 - accuracy: 0.9974 - val_loss: 1.1913e-04 - val_accuracy: 1.0000\n",
      "Epoch 30/50\n",
      "25/25 [==============================] - 35s 1s/step - loss: 0.0372 - accuracy: 0.9909 - val_loss: 2.5988e-04 - val_accuracy: 1.0000\n",
      "Epoch 31/50\n",
      "25/25 [==============================] - 37s 1s/step - loss: 0.0192 - accuracy: 0.9961 - val_loss: 0.0015 - val_accuracy: 1.0000\n",
      "Epoch 32/50\n",
      "25/25 [==============================] - 37s 2s/step - loss: 0.0034 - accuracy: 1.0000 - val_loss: 8.5597e-05 - val_accuracy: 1.0000\n",
      "Epoch 33/50\n",
      "25/25 [==============================] - 43s 2s/step - loss: 0.0110 - accuracy: 0.9987 - val_loss: 7.6609e-05 - val_accuracy: 1.0000\n",
      "Epoch 34/50\n",
      "25/25 [==============================] - 45s 2s/step - loss: 0.0103 - accuracy: 0.9961 - val_loss: 2.0301e-04 - val_accuracy: 1.0000\n",
      "Epoch 35/50\n",
      "25/25 [==============================] - 47s 2s/step - loss: 0.0247 - accuracy: 0.9948 - val_loss: 0.0058 - val_accuracy: 1.0000\n",
      "Epoch 36/50\n",
      "25/25 [==============================] - 42s 2s/step - loss: 0.0140 - accuracy: 0.9948 - val_loss: 5.4132e-04 - val_accuracy: 1.0000\n",
      "Epoch 37/50\n",
      "25/25 [==============================] - 47s 2s/step - loss: 0.0128 - accuracy: 0.9974 - val_loss: 6.0789e-04 - val_accuracy: 1.0000\n",
      "Epoch 38/50\n",
      "25/25 [==============================] - 46s 2s/step - loss: 0.0091 - accuracy: 0.9987 - val_loss: 5.8008e-04 - val_accuracy: 1.0000\n",
      "Epoch 39/50\n",
      "25/25 [==============================] - 38s 2s/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 5.0070e-05 - val_accuracy: 1.0000\n",
      "Epoch 40/50\n",
      "25/25 [==============================] - 36s 1s/step - loss: 1.3785e-04 - accuracy: 1.0000 - val_loss: 1.7187e-05 - val_accuracy: 1.0000\n",
      "Epoch 41/50\n",
      "25/25 [==============================] - 36s 1s/step - loss: 1.6763e-04 - accuracy: 1.0000 - val_loss: 9.4056e-06 - val_accuracy: 1.0000\n",
      "Epoch 42/50\n",
      "25/25 [==============================] - 42s 2s/step - loss: 0.0071 - accuracy: 0.9987 - val_loss: 2.0063e-05 - val_accuracy: 1.0000\n",
      "Epoch 43/50\n",
      "25/25 [==============================] - 37s 2s/step - loss: 0.0051 - accuracy: 0.9974 - val_loss: 2.6497e-04 - val_accuracy: 1.0000\n",
      "Epoch 44/50\n",
      "25/25 [==============================] - 36s 1s/step - loss: 0.0213 - accuracy: 0.9974 - val_loss: 0.0017 - val_accuracy: 1.0000\n",
      "Epoch 45/50\n",
      "25/25 [==============================] - 36s 1s/step - loss: 0.0060 - accuracy: 0.9974 - val_loss: 2.5869e-05 - val_accuracy: 1.0000\n",
      "Epoch 46/50\n",
      "25/25 [==============================] - 37s 2s/step - loss: 0.0111 - accuracy: 0.9961 - val_loss: 3.0841e-04 - val_accuracy: 1.0000\n",
      "Epoch 47/50\n",
      "25/25 [==============================] - 36s 1s/step - loss: 0.0114 - accuracy: 0.9961 - val_loss: 1.2310e-04 - val_accuracy: 1.0000\n",
      "Epoch 48/50\n",
      "25/25 [==============================] - 36s 1s/step - loss: 0.0040 - accuracy: 0.9974 - val_loss: 1.5114e-04 - val_accuracy: 1.0000\n",
      "Epoch 49/50\n",
      "25/25 [==============================] - 35s 1s/step - loss: 0.0340 - accuracy: 0.9909 - val_loss: 0.0022 - val_accuracy: 1.0000\n",
      "Epoch 50/50\n",
      "25/25 [==============================] - 38s 2s/step - loss: 0.0100 - accuracy: 0.9961 - val_loss: 9.6389e-05 - val_accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "%timeit \n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_val, y_val),\n",
    "    callbacks=[tensorboard_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get training and validation loss from history object\n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "# Create an array of epochs\n",
    "epochs = range(1, len(train_loss) + 1)\n",
    "\n",
    "# Plot training and validation loss\n",
    "plt.figure(figsize=(5.2, 5.2))\n",
    "plt.plot(epochs, train_loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.yscale('log') \n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Save the plot as a PNG file\n",
    "plt.savefig('training_validation_loss_50epochs.png')\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start tesnorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir logs/fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model on the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss, val_accuracy = model.evaluate(X_val, y_val)\n",
    "print(f'Validation accuracy: {val_accuracy:.4f}')\n",
    "\n",
    "# Save the model\n",
    "model.save('dog_breed_classifier_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict breed of a random dog from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_random_image(model, X, y, label_encoder, save_dir):\n",
    "    # Pick a random image from our dataset\n",
    "    idx = random.randint(0, len(X) - 1)\n",
    "    \n",
    "    # Predict the class of the selected image\n",
    "    y_pred = model.predict(X[idx].reshape(1, 150, 150, 3))\n",
    "    print(f'Prediction probabilities: {y_pred}')\n",
    "    predicted_class = label_encoder.inverse_transform([np.argmax(y_pred)])\n",
    "    \n",
    "    img = Image.fromarray((X[idx] * 255).astype(np.uint8))\n",
    "    # Resize the image to the desired output size\n",
    "    output_size = (500,500)\n",
    "    img = img.resize(output_size, Image.ANTIALIAS)\n",
    "    \n",
    "    plt.imshow(X[idx])\n",
    "    plt.title(f'Actual: {label_encoder.inverse_transform([np.argmax(y[idx])])[0]}')\n",
    "    plt.show()\n",
    "    \n",
    "    # Draw the predicted class on the image\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    font_size = 30\n",
    "    font_path = os.path.join(cv2.__path__[0],'qt','fonts','DejaVuSans.ttf')\n",
    "    font = ImageFont.truetype(font_path, font_size)\n",
    "    text = f'Predicted: {predicted_class[0]}'\n",
    "    textwidth, textheight = draw.textsize(text, font)\n",
    "    width, height = img.size\n",
    "    margin = 10\n",
    "    x = width - textwidth - margin\n",
    "    y = height - textheight - margin\n",
    "\n",
    "    draw.text((x, y), text, font=font, fill=(255, 255, 255, 255))\n",
    "    \n",
    "    save_path = os.path.join(save_dir, f'predicted_{idx}_Orig.png')\n",
    "    img.save(save_path)\n",
    "\n",
    "    # Get the predicted class\n",
    "    predicted_class = label_encoder.inverse_transform([np.argmax(y_pred)])\n",
    "    print(f'Predicted class: {predicted_class[0]}')\n",
    "\n",
    "# Load the trained model\n",
    "model = load_model('dog_breed_classifier_model.h5')\n",
    "\n",
    "pred_dir = '/home/prasad/09_personal/Coding_N_Work/2_ML_NN/Computer_Vision_data_Explore/Computer-Vision-Classify-Dogs/zzz_my_predictions/'\n",
    "\n",
    "# Predict for a random image\n",
    "predict_random_image(model, X_val, y_val, label_encoder, pred_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get training and validation loss from history object\n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "# Create an array of epochs\n",
    "epochs = range(1, len(train_loss) + 1)\n",
    "\n",
    "# Plot training and validation loss\n",
    "plt.figure(figsize=(5.2, 5.2))\n",
    "plt.plot(epochs, train_loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.yscale('log') \n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Save the plot as a PNG file\n",
    "plt.savefig('training_validation_loss.png')\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here I test actual pictures of my dog and see if my model identifies it\n",
    "She is labrador btw ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dir = '/home/prasad/09_personal/Coding_N_Work/2_ML_NN/Computer_Vision_data_Explore/Computer-Vision-Classify-Dogs/Test/'\n",
    "\n",
    "# Check the input shape of the model\n",
    "input_shape = model.layers[0].input_shape\n",
    "img_height, img_width = input_shape[1], input_shape[2]\n",
    "target_image_size=(img_height, img_width)\n",
    "\n",
    "# Iterate through the images in the test directory and process them\n",
    "test_images = []\n",
    "for filename in os.listdir(test_dir):\n",
    "    img_path = os.path.join(test_dir, filename)\n",
    "    img = Image.open(img_path)\n",
    "    img = img.resize(target_image_size)\n",
    "    img = np.array(img)\n",
    "    img = img / 255.0 # Normalize the image array\n",
    "    test_images.append(img)\n",
    "\n",
    "len(test_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make predictions for my dog "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_my_test_image(model, X, label_encoder, idx, save_dir, training_images):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    # Predict the class of the selected image\n",
    "    y_pred = model.predict(X[idx].reshape(1, 150, 150, 3))\n",
    "    # print(f'Prediction probabilities: {y_pred}')\n",
    "\n",
    "    # Get the predicted class\n",
    "    predicted_class = label_encoder.inverse_transform([np.argmax(y_pred)])\n",
    "    print(f'Predicted class: {predicted_class[0]}')\n",
    "\n",
    "    # idx = random.randint(0, len(X) - 1)\n",
    "    # plt.imshow(X[idx])\n",
    "    # plt.show()\n",
    "\n",
    "    # Convert numpy array to PIL image\n",
    "    img = Image.fromarray((X[idx] * 255).astype(np.uint8))\n",
    "    # Resize the image to the desired output size\n",
    "    output_size = (500,500)\n",
    "    img = img.resize(output_size, Image.ANTIALIAS)\n",
    "\n",
    "    # Draw the predicted class on the image\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    font_size = 30\n",
    "    font_path = os.path.join(cv2.__path__[0],'qt','fonts','DejaVuSans.ttf')\n",
    "    font = ImageFont.truetype(font_path, font_size)\n",
    "    text = f'Predicted: {predicted_class[0]}'\n",
    "    textwidth, textheight = draw.textsize(text, font)\n",
    "    width, height = img.size\n",
    "    margin = 10\n",
    "    x = width - textwidth - margin\n",
    "    y = height - textheight - margin\n",
    "\n",
    "    draw.text((x, y), text, font=font, fill=(255, 255, 255, 255))\n",
    "\n",
    "    # # Find the best matching image from training dataset X\n",
    "    # best_match_idx = np.argmax(y_pred)\n",
    "    # best_match_img = Image.fromarray((training_images[best_match_idx] * 255).astype(np.uint8))\n",
    "    # best_match_img = best_match_img.resize(output_size, Image.ANTIALIAS)\n",
    "\n",
    "    # # Create a new image with the original and the best match side by side\n",
    "    # comparison_img = Image.new('RGB', (2 * output_size[0], output_size[1]))\n",
    "    # comparison_img.paste(img, (0, 0))\n",
    "    # comparison_img.paste(best_match_img, (output_size[0], 0))\n",
    "\n",
    "    # # Save the comparison image with prediction text\n",
    "    # save_path = os.path.join(save_dir, f'comparison_{idx}.png')\n",
    "    # comparison_img.save(save_path)\n",
    "    # print(f'Saved comparison image with prediction at {save_path}')\n",
    "\n",
    "    # Save the image with prediction text\n",
    "    save_path = os.path.join(save_dir, f'predicted_{idx}_myDog.png')\n",
    "    img.save(save_path)\n",
    "    print(f'Saved image with prediction at {save_path}')\n",
    "\n",
    "# Load the trained model\n",
    "model = load_model('dog_breed_classifier_model.h5')\n",
    "\n",
    "# Predict for a random image\n",
    "for img_id in range(len(test_images)):\n",
    "    predict_my_test_image(model, test_images, label_encoder, img_id, pred_dir, X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get training and validation loss from history object\n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "# Create an array of epochs\n",
    "epochs = range(1, len(train_loss) + 1)\n",
    "\n",
    "# Plot training and validation loss\n",
    "plt.plot(epochs, train_loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.yscale('log') \n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Save the plot as a PNG file\n",
    "plt.savefig('training_validation_loss.png')\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
